name: PR Review with DeepSeek

on:
  pull_request:
    types: [opened, synchronize]

jobs:
  test-ollama:
    runs-on: ubuntu-latest
    outputs:
      model_response: ${{ steps.run-model.outputs.response }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Get PR changes
        id: pr-diff
        run: |
          echo "=== Getting PR Changes ==="

          echo "Getting changed files..."
          echo "Changed files:" > pr_changes.txt
          git diff --name-status ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }} | tee -a pr_changes.txt

          echo -e "\nGetting detailed changes..."
          echo -e "\nDetailed changes:" >> pr_changes.txt
          git diff ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }} | tee -a pr_changes.txt

          echo -e "\n=== Full PR Changes Content ==="
          cat pr_changes.txt

          # Store in environment variable
          {
            echo 'PR_CHANGES<<EOF'
            cat pr_changes.txt
            echo 'EOF'
          } >> $GITHUB_ENV

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Read custom prompt
        id: read-prompt
        run: |
          if [ -f ".github/workflows/pr_review_prompt.txt" ]; then
            {
              echo 'CUSTOM_PROMPT<<EOF'
              cat .github/workflows/pr_review_prompt.txt
              echo 'EOF'
            } >> $GITHUB_ENV
          else
            echo "Warning: Custom prompt file not found. Using default prompt."
            echo "CUSTOM_PROMPT=Please review the following pull request changes and provide feedback." >> $GITHUB_ENV
          fi

      - name: Run DeepSeek model
        id: run-model
        run: |
          # Combine prompt and changes
          echo "$CUSTOM_PROMPT" > full_prompt.txt
          echo -e "\nChanges to review:\n$PR_CHANGES" >> full_prompt.txt

          # Run the model and capture complete response
          response=$(cat full_prompt.txt | ollama run deepseek-r1:1.5b 2>/dev/null |
            grep -v '^[0-9]' |
            grep -v '25[hl]' |
            grep -v '⠙\|⠸\|⠴\|⠦\|⠧\|⠇\|⠏\|⠋\|⠙\|⠹\|⠼' |
            sed '/^$/d' |
            tr '\n' '\r' |
            sed 's/\r/\\n/g')

          # Create formatted output with headers and separators
          formatted_output="::group::Model Response\\n"
          formatted_output+="$response\\n"
          formatted_output+="::endgroup::"

          echo -e "$formatted_output"

          # Set output for next job
          echo "response<<EOF" >> $GITHUB_OUTPUT
          echo "$response" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

  comment-on-pr:
    needs: test-ollama
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write

    steps:
      - name: Comment AI Review on PR
        uses: actions/github-script@v7
        with:
          script: |
            const aiReview = `# AI Code Review

            ${{ needs.test-ollama.outputs.model_response }}

            ---
            *This review was automatically generated by DeepSeek AI.*`;
            
            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: aiReview
            });